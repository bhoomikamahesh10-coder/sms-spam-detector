{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK4qI12QOCco"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Step 1: Install dependencies (if not already available in Colab)\n",
        "!pip install nltk\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Step 3: Load Dataset\n",
        "# Dataset link: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
        "# You can also upload a CSV file in Colab\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
        "df = pd.read_csv(url, sep=\"\\t\", header=None, names=[\"label\", \"message\"])\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Step 4: Preprocess Data\n",
        "def clean_text(msg):\n",
        "    msg = msg.lower()  # lowercase\n",
        "    msg = \"\".join([c for c in msg if c not in string.punctuation])  # remove punctuation\n",
        "    return msg\n",
        "\n",
        "df[\"message\"] = df[\"message\"].apply(clean_text)\n",
        "\n",
        "# Step 5: Split Data\n",
        "X = df[\"message\"]\n",
        "y = df[\"label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Vectorize (TF-IDF)\n",
        "vectorizer = TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 7: Train Model (Naive Bayes)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Step 8: Evaluate\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(\"\\nâœ… Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nðŸ“Š Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nðŸ“‰ Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Step 9: Test Custom Messages\n",
        "test_msgs = [\n",
        "    \"Congratulations! You've won a $1000 Walmart gift card. Click here to claim now!\",\n",
        "    \"Hey, are we still meeting for lunch today?\",\n",
        "    \"URGENT! Your account has been suspended. Verify immediately.\"\n",
        "]\n",
        "\n",
        "test_vec = vectorizer.transform(test_msgs)\n",
        "predictions = model.predict(test_vec)\n",
        "\n",
        "for msg, pred in zip(test_msgs, predictions):\n",
        "    print(f\"\\nMessage: {msg}\\nPrediction: {pred}\")"
      ]
    }
  ]
}